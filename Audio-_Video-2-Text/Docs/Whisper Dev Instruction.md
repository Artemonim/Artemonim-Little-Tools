## Инструкция по взаимодействию с локальным OpenAI Whisper API

Этот документ описывает пошаговый процесс настройки и взаимодействия с локально запущенным сервером OpenAI Whisper API. Инструкция предназначена для разработчиков, создающих инструмент, отправляющий аудио на Whisper и получающий текстовую транскрипцию.

---

### 1. Предварительные требования

1. **Аппаратные ресурсы**:

   * CPU с поддержкой AVX2/AVX512 или GPU (опционально для ускорения).
2. **Операционная система**: Linux, macOS или Windows.
3. **Установленный Python** (версии 3.8 и выше).
4. **Git-клиент** для клонирования репозитория Whisper (опционально).

---

### 2. Установка и запуск локального сервера

1. **Установка зависимостей**:

   ```bash
   pip install openai-whisper uvicorn fastapi python-multipart
   ```
2. **Запуск простого HTTP-сервера**:

   * Создайте файл `server.py` со следующим содержимым:

     ```python
     from fastapi import FastAPI, File, UploadFile, Form
     import whisper

     app = FastAPI()
     model = whisper.load_model("base")  # или small, medium, large

     @app.post("/v1/audio/transcriptions")
     async def transcribe(
         file: UploadFile = File(...),
         model_name: str = Form("base"),
         temperature: float = Form(0.0),
         language: str = Form(None)
     ):
         # Чтение аудио в байты
         audio_bytes = await file.read()
         result = model.transcribe(audio_bytes, temperature=temperature, language=language)
         return {"text": result["text"]}
     ```
   * Запустите сервер:

     ```bash
     uvicorn server:app --host 127.0.0.1 --port 8000
     ```
   * После этого локальный API доступен по адресу `http://127.0.0.1:8000`.

Успешный ответ (200 OK):

```json
{
  "text": "Текстовая транскрипция аудиозаписи"
}
```

Ошибки:

400 Bad Request — неверный формат данных.

500 Internal Server Error — ошибка сервера или модели.

---

### 2.1. Актуальные доступные модели Whisper и их характеристики

| Модель             | Размер файла (≈) | Параметры (≈) | Многоязычность | Рекомендуемая память RAM/VRAM | Скорость (относительно base на CPU) |
| ------------------ | ---------------- | ------------- | -------------- | ----------------------------- | ----------------------------------- |
| tiny               | 40 MB            | 39 M          | ✓              | ≥ 1 GB                        | 4×                                  |
| base               | 75 MB            | 74 M          | ✓              | ≥ 1.5 GB                      | 2×                                  |
| small              | 244 MB           | 244 M         | ✓              | ≥ 2.5 GB                      | 1×                                  |
| medium             | 786 MB           | 769 M         | ✓              | ≥ 5 GB                        | 0.5×                                |
| large              | 2.9 GB           | 2.9 B         | ✓              | ≥ 10 GB                       | 0.2×                                |
| **large-v3-turbo** | \~1.1 GB         | \~809 M       | ✓              | ≥ 6 GB                        | **2.2×**                            |

#### large-v3-turbo:

* **Размер файла** — примерно вдвое меньше `large`, благодаря архитектурным оптимизациям.
* **Параметры** — чуть больше, чем у `medium`, но меньше оригинального `large`.
* **Скорость** — значительно выше даже по сравнению с `small`, за счёт агрессивного сжатия декодера и улучшений в инференсе.
* **Качество** — практически на уровне `large-v3`, точнее `medium`, особенно при шумных или многоязычных вводах.

### 2.2. Влияние параметра temperature

Параметр temperature управляет степенью случайности при выборе токенов во время декодирования модели:

temperature = 0.0: детерминированное поведение — модель всегда выбирает наиболее вероятный токен. Рекомендуется для максимальной точности и воспроизводимости результатов.

0.0 < temperature ≤ 0.5: низкая случайность — модель может выбирать редкие варианты, но преимущественно остаётся консервативной. Позволяет учитывать альтернативные варианты распознавания при небольшой потере точности.

0.5 < temperature ≤ 1.0: высокая случайность — модель генерирует более разнообразные варианты, что может быть полезно для шумных записей или творческой обработки, но точность базовой транскрипции может снижаться.

Рекомендации по выбору temperature:

Для стандартных задач транскрипции речи без фонового шума используйте 0.0.

При плохом качестве аудио или шуме попробуйте 0.2–0.4, чтобы модель допустила больше вариаций в выборе токенов.

Значения выше 0.5 подходят для экспериментов или генерации альтернативных текстовых гипотез, но не гарантируют точности.

### 3. Описание API

| Метод | Путь                       | Параметры формы                                                                                 | Описание                                                  |
| ----- | -------------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| POST  | `/v1/audio/transcriptions` | `file` (UploadFile), `model_name` (string),`temperature` (float), `language` (string, optional) | Отправляет аудиофайл и возвращает текстовую транскрипцию. |

1. **Заголовки запроса**:

   * `Content-Type: multipart/form-data`
2. **Тело запроса**:

   * Поле `file` — аудиофайл формата WAV, MP3, MP4 или другого, поддерживаемого Whisper.
   * Поле `model_name` — имя модели ("base", "small", "medium", "large").
   * Поле `temperature` — значение в диапазоне \[0.0, 1.0] (по умолчанию 0.0).
   * Поле `language` — код языка (ISO 639-1), например, `ru` для русского (опционально).
3. **Успешный ответ** (`200 OK`):

   ```json
   {
     "text": "Текстовая транскрипция аудиозаписи"
   }
   ```
4. **Ошибки**:

   * `400 Bad Request` — неверный формат данных.
   * `500 Internal Server Error` — ошибка сервера или модели.

---

### 4. Примеры использования

#### 4.1. cURL

```bash
curl -X POST \
  "http://127.0.0.1:8000/v1/audio/transcriptions" \
  -F file=@path/to/audio.mp3 \
  -F model_name=base \
  -F temperature=0.0 \
  -F language=ru
```

#### 4.2. Python (requests)

```python
import requests

url = "http://127.0.0.1:8000/v1/audio/transcriptions"
files = {"file": open("audio.mp3", "rb")}
data = {"model_name": "base", "temperature": 0.0, "language": "ru"}
response = requests.post(url, files=files, data=data)
print(response.json()["text"])
```

---

### 5. Рекомендации по оптимизации

* Используйте `small` или `medium` модели для ускоренной обработки на CPU.
* Для GPU-инференса установите `torch` с CUDA и запускайте в окружении, где доступна GPU.
* Обрабатывайте аудио по частям (deprecated), если файл слишком большой.

---

### 6. Обработка ошибок и логирование

1. Логируйте входящие запросы и ошибки модели:

   ```python
   import logging
   logging.basicConfig(level=logging.INFO)
   logger = logging.getLogger(__name__)

   # Внутри эндпоинта:
   logger.info(f"Received file: {file.filename}, size: {len(audio_bytes)} bytes")
   ```
2. Возвращайте понятные сообщения об ошибках клиенту.

---

### 7. Безопасность

* Разрешайте доступ только с `localhost` или через VPN.
* Ограничьте размер загружаемых файлов (например, до 100 МБ).
* Внедрите аутентификацию (API-ключ), если планируется общий доступ.

---

*Документ подготовлен для команды разработки. При необходимости обновляйте разделы при изменении API или инфраструктуры.*
