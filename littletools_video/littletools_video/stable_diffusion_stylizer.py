#!/usr/bin/env python3
# -*- coding: utf-8 -*-


"""
    --- AUTO-GENERATED DOCSTRING ---
    This docstring is automatically generated by Agent Docstrings.
    Do not modify this block directly.

    Classes/Functions:
      - Functions:
        - validate_model_compatibility(model_id: str, controlnet_id: Optional[str]) -> bool (line 145)
        - clear_pipeline_cache() (line 221)
        - get_resolution_dimensions(resolution: str) -> Tuple[int, int] (line 230)
        - create_worker_pipeline() -> Any (line 523)
        - get_video_framerate(input_video: Path) -> float (line 781)
        - upscale_frames_realesrgan(stylized_dir: Path, upscaled_dir: Path) -> bool (line 919)
    --- END AUTO-GENERATED DOCSTRING ---
Stable Diffusion Video Stylizer

This tool stylizes videos using Stable Diffusion models and upscales them using Real-ESRGAN.
Optimized for 8GB VRAM with memory-efficient techniques.

+ALGORITHM OVERVIEW:
+==================
+
+The video stylization process follows these main stages:
+
+1. FRAME EXTRACTION
+   - Extract individual frames from input video using FFmpeg
+   - Scale frames to target resolution (480p/720p/1080p) with Lanczos filtering
+   - Optional: Process only first 2 seconds in test mode
+   - Save frames as high-quality PNG files (quality level 2)
+
+2. MODEL LOADING & PIPELINE SETUP
+   - Download Stable Diffusion model from HuggingFace Hub if needed
+   - Initialize master pipeline with memory optimizations:
+     * Enable attention slicing for VRAM efficiency
+     * Use XFormers memory-efficient attention when available
+     * Configure scheduler (UniPC/DDIM/EulerA/DPM2M)
+   - Create worker pipeline clones for concurrent processing
+   - Optional: Load and integrate ControlNet for structural guidance
+
+3. STYLIZATION MODES
+
+   A) ANIMATEDIFF MODE (Temporal Consistency):
+      - Load AnimateDiff motion adapter for temporal coherence
+      - Process video in overlapping chunks (16-frame context window)
+      - Apply ControlNet preprocessing if enabled:
+        * Canny: Edge detection for structural preservation
+        * Depth: MiDaS depth estimation for 3D structure
+        * OpenPose: Human pose detection for character consistency
+      - Generate stylized chunks with frame blending at overlaps
+      - Maintain temporal smoothness across chunk boundaries
+
+   B) PER-FRAME MODE (Independent Processing):
+      - Process each frame independently using Img2Img pipeline
+      - Use semaphore-controlled concurrency for memory management
+      - Apply stylization with configurable strength (0.0-1.0)
+      - Clear VRAM cache after each frame to prevent OOM
+
+4. UPSCALING (Optional)
+   - Load Real-ESRGAN x4plus model for super-resolution
+   - Process stylized frames with 4x upscaling
+   - Use FP16 precision and tiling for memory efficiency
+   - Output high-resolution frames ready for reassembly
+
+5. VIDEO REASSEMBLY
+   - Detect original video framerate using FFprobe
+   - Encode stylized frames using HEVC (H.265) with CQ=26
+   - Copy original audio tracks without re-encoding
+   - Apply faststart flag for web streaming compatibility
+   - Ensure output duration matches shortest stream
+
+MEMORY OPTIMIZATION STRATEGIES:
+==============================
+
+- Master/Worker Pipeline Pattern: Single model in memory, multiple inference heads
+- Attention Slicing: Reduces peak VRAM usage during attention computation
+- XFormers Integration: Memory-efficient attention implementation
+- Progressive VRAM Clearing: torch.cuda.empty_cache() after each frame
+- FP16 Precision: Halves memory usage for compatible GPUs
+- Semaphore Concurrency Control: Prevents memory overload from parallel workers
+- Chunk-based AnimateDiff: Processes long videos in manageable segments
+
+INTERACTIVE FEATURES:
+====================
+
+- Automatic mode detection: Triggers interactive menu when key parameters missing
+- Rich CLI interface with progress bars and colored output
+- Settings menu for scheduler, ControlNet, upscaling, and test mode
+- Path validation and error handling with user-friendly messages
+- Cancellation support with proper cleanup of background tasks
+
+SUPPORTED FORMATS & MODELS:
+===========================
+
+Input: Any FFmpeg-supported video format (MP4, AVI, MOV, MKV, etc.)
+Output: MP4 with HEVC video + original audio
+
+Base Models:
+- Stable Diffusion 1.5: runwayml/stable-diffusion-v1-5 (768-dim cross-attention)
+- Stable Diffusion 2.x: stabilityai/stable-diffusion-2-base (1024-dim cross-attention)
+
+ControlNets:
+- SD 1.5 compatible: lllyasviel/sd-controlnet-{canny,depth,openpose}
+- SD 2.x compatible: thibaud/controlnet-sd21-{canny,depth,openpose}-diffusers
+
+! IMPORTANT: Base model and ControlNet must have matching cross-attention dimensions
+! to avoid "mat1 and mat2 shapes cannot be multiplied" errors.
+
+Upscaler: Real-ESRGAN x4plus for 4x super-resolution
+Schedulers: UniPC (default), DDIM, Euler Ancestral, DPM++ 2M
+
+PERFORMANCE CHARACTERISTICS:
+============================
+
+- Memory Usage: ~6-8GB VRAM for 512x512 processing
+- Processing Speed: ~2-5 seconds per frame (GPU dependent)
+- AnimateDiff: 2-3x slower but significantly better temporal consistency
+- Upscaling: Additional ~1-2 seconds per frame
+- Recommended: RTX 3070/4060 or better for smooth operation
"""

import asyncio
import contextlib
import functools
import os
import random  # For reproducible seed selection
import tempfile

# * Suppress xformers FutureWarnings and import required modules for stderr redirection
import warnings
from pathlib import Path
from typing import Any
from typing import Optional
from typing import Tuple

import torch
import typer
from rich.console import Console
from rich.progress import BarColumn
from rich.progress import Progress
from rich.progress import TaskID
from rich.progress import TextColumn
from rich.progress import TimeRemainingColumn
from typing_extensions import Annotated

warnings.filterwarnings("ignore", category=FutureWarning, module="xformers")

# * Add Real-ESRGAN compatibility fix for newer torchvision versions
try:
    # * Attempt to import the deprecated module
    import torchvision.transforms.functional_tensor  # type: ignore[import-untyped]
except ImportError:
    # * Create a monkey-patch for missing functional_tensor module
    import sys
    import types

    # * Create a mock module to avoid import errors
    mock_module = types.ModuleType("functional_tensor")

    # * Add essential functions that Real-ESRGAN might need
    def rgb_to_grayscale(tensor: Any, num_output_channels: int = 1) -> Any:
        """Mock implementation of rgb_to_grayscale"""
        try:
            import torchvision.transforms.functional as F

            return F.rgb_to_grayscale(tensor, num_output_channels)
        except ImportError:
            # * Fallback implementation if torch is not available
            return tensor

    # * Add the function to the mock module using setattr
    setattr(mock_module, "rgb_to_grayscale", rgb_to_grayscale)

    # * Inject the mock module into sys.modules
    sys.modules["torchvision.transforms.functional_tensor"] = mock_module

# * Suppress excessive info logs (e.g., "Keyword arguments {...}") coming from diffusers internals
# * by elevating the diffusers logger threshold to ERROR.
import logging

from littletools_core.huggingface_utils import download_hf_model
from littletools_core.utils import ensure_dir_exists
from littletools_core.utils import prompt_for_interactive_settings
from littletools_core.utils import prompt_for_path

# * Delay heavy diffusers imports until necessary functions to optimize startup and avoid unused-import warnings.
# * Only import what is required at runtime within functions.


logging.getLogger("diffusers").setLevel(logging.ERROR)

from controlnet_aux import CannyDetector
from controlnet_aux import MidasDetector
from controlnet_aux import OpenposeDetector

# * 'Compel' automatic prompt embedding is not currently used in this script. Remove unused import to satisfy linters.
from PIL import Image

# * Global configuration
console = Console()

# * Log the Real-ESRGAN compatibility patch if it was applied
if "torchvision.transforms.functional_tensor" in sys.modules:
    try:
        import torchvision.transforms.functional_tensor  # type: ignore[import-untyped]
    except ImportError:
        console.print("[yellow]! Applied compatibility patch for Real-ESRGAN[/yellow]")

app = typer.Typer(
    name="stylize",
    help="Stylize videos using Stable Diffusion and upscale with Real-ESRGAN.",
    no_args_is_help=True,
)

# * Enable torch.compile() caching for better performance
os.environ["TORCH_COMPILE_CACHE_DIR"] = os.path.join(
    os.getcwd(), ".torch_compile_cache"
)
os.environ["TORCHINDUCTOR_CACHE_DIR"] = os.path.join(
    os.getcwd(), ".torchinductor_cache"
)

# * Set torch matmul precision for better performance on Ampere+ GPUs
try:
    if torch.cuda.is_available():
        torch.set_float32_matmul_precision("high")
except ImportError:
    pass  # torch not installed yet

# * Global variable to store the master compiled pipeline
_compiled_master_pipeline = None

# * Silence noisy library warnings and backtraces -------------------------------
os.environ.setdefault(
    "XFORMERS_DISABLE_TRITON_WARNING", "1"
)  # * Suppress xformers Triton traceback

# * Filter out specific warning categories from third-party libs
warnings.filterwarnings("ignore", category=UserWarning, module="timm")
warnings.filterwarnings("ignore", category=UserWarning, module="xformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="controlnet_aux")
warnings.filterwarnings("ignore", category=UserWarning, module="diffusers")

# * Add top-level definition for ControlNet video2video pipeline to satisfy static analysis
try:
    from diffusers import AnimateDiffVideoToVideoControlNetPipeline  # type: ignore
except ImportError:
    AnimateDiffVideoToVideoControlNetPipeline = None  # type: ignore


def validate_model_compatibility(model_id: str, controlnet_id: Optional[str]) -> bool:
    """
    Validate compatibility between base model and ControlNet.

    Args:
        model_id: Base Stable Diffusion model ID
        controlnet_id: ControlNet model ID

    Returns:
        True if models are compatible, False otherwise
    """
    if not controlnet_id:
        return True  # No ControlNet, no compatibility issues

    # * SD 1.5 models (768-dim cross-attention)
    sd15_models = [
        "runwayml/stable-diffusion-v1-5",
        "stabilityai/stable-diffusion-xl-base-1.0",  # Note: SDXL has different requirements
        "dreamlike-art/dreamlike-photoreal-2.0",
        "SG161222/Realistic_Vision_V2.0",
    ]

    # * SD 2.x models (1024-dim cross-attention)
    sd2_models = [
        "stabilityai/stable-diffusion-2-base",
        "stabilityai/stable-diffusion-2-1-base",
        "stabilityai/stable-diffusion-2",
        "stabilityai/stable-diffusion-2-1",
    ]

    # * Standard ControlNet models (designed for SD 1.5)
    sd15_controlnets = [
        "lllyasviel/sd-controlnet-canny",
        "lllyasviel/sd-controlnet-depth",
        "lllyasviel/sd-controlnet-openpose",
        "lllyasviel/sd-controlnet-scribble",
        "lllyasviel/sd-controlnet-seg",
        "lllyasviel/control_v11p_sd15_canny",
        "lllyasviel/control_v11f1p_sd15_depth",
        "lllyasviel/control_v11p_sd15_openpose",
    ]

    # * SD 2.x ControlNet models
    sd2_controlnets = [
        "thibaud/controlnet-sd21-canny-diffusers",
        "thibaud/controlnet-sd21-depth-diffusers",
        "thibaud/controlnet-sd21-openpose-diffusers",
    ]

    is_sd15_model = any(model in model_id for model in sd15_models)
    is_sd2_model = any(model in model_id for model in sd2_models)
    is_sd15_controlnet = any(cn in controlnet_id for cn in sd15_controlnets)
    is_sd2_controlnet = any(cn in controlnet_id for cn in sd2_controlnets)

    # * Check compatibility
    if is_sd15_model and is_sd15_controlnet:
        return True
    elif is_sd2_model and is_sd2_controlnet:
        return True
    elif is_sd15_model and is_sd2_controlnet:
        console.print(
            f"[yellow]! Warning: SD 1.5 model with SD 2.x ControlNet may cause dimension mismatch[/yellow]"
        )
        return False
    elif is_sd2_model and is_sd15_controlnet:
        console.print(
            f"[yellow]! Warning: SD 2.x model with SD 1.5 ControlNet may cause dimension mismatch[/yellow]"
        )
        return False
    else:
        console.print(
            f"[yellow]! Warning: Unknown model combination, proceeding with caution[/yellow]"
        )
        return True  # Allow unknown combinations but warn


def clear_pipeline_cache():
    """Clear the cached master pipeline to free memory."""
    global _compiled_master_pipeline
    if _compiled_master_pipeline is not None:
        console.print("[*] Clearing cached master pipeline...")
        _compiled_master_pipeline = None
        console.print("[green]âœ“ Pipeline cache cleared[/green]")


def get_resolution_dimensions(resolution: str) -> Tuple[int, int]:
    """
    Convert resolution string to width, height tuple.

    Args:
        resolution: Resolution string like '480p', '720p', '1080p'

    Returns:
        Tuple of (width, height)
    """
    resolution_map = {
        "480p": (854, 480),
        "720p": (1280, 720),
        "1080p": (1920, 1080),
    }
    if resolution not in resolution_map:
        raise ValueError(f"Unsupported resolution: {resolution}")
    return resolution_map[resolution]


async def extract_frames(
    input_video: Path,
    frames_dir: Path,
    resolution: str = "480p",
    test_mode: bool = False,
    fps: int = 15,
) -> bool:
    """
    Extract frames from video at specified resolution.

    Args:
        input_video: Path to the input video file
        frames_dir: Directory to save extracted frames
        resolution: Target resolution for extraction
        test_mode: If True, only process the first 2 seconds.
        fps: Frame rate to use for extraction

    Returns:
        True if successful, False otherwise
    """
    try:
        # * Ensure frames directory exists
        ensure_dir_exists(frames_dir)

        # * Get target dimensions
        width, height = get_resolution_dimensions(resolution)

        console.print(f"[*] Extracting frames at {resolution} ({width}x{height})...")
        if test_mode:
            console.print(
                "[yellow]! Test mode enabled: processing first 2 seconds only.[/yellow]"
            )

        # * Build ffmpeg command for frame extraction
        ffmpeg_cmd = [
            "ffmpeg",
            "-y",  # Overwrite output files
            "-i",
            str(input_video),
            "-r",
            str(fps),  # Limit FPS
            "-vf",
            f"scale={width}:{height}:flags=lanczos",
            "-q:v",
            "2",  # High quality JPEG
        ]

        # * If in test mode, only extract first 2 seconds
        if test_mode:
            ffmpeg_cmd.extend(["-t", "2"])

        ffmpeg_cmd.append(str(frames_dir / "%06d.png"))

        # * Run ffmpeg command
        proc = await asyncio.create_subprocess_exec(
            *ffmpeg_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode != 0:
            console.print(f"[red]! FFmpeg error: {stderr.decode()}[/red]")
            return False

        # * Count extracted frames
        frame_files = list(frames_dir.glob("*.png"))
        console.print(f"[green]âœ“ Extracted {len(frame_files)} frames[/green]")
        return True

    except Exception as e:
        console.print(f"[red]! Error extracting frames: {e}[/red]")
        return False


def setup_master_compiled_pipeline(
    model_id: str,
    scheduler_name: str = "UniPC",
    med_vram: bool = False,
    low_vram: bool = False,
    controlnet_id: Optional[str] = None,
) -> Any:
    """
    Set up and compile the master Stable Diffusion pipeline.

    On CUDA: the pipeline is loaded directly on GPU with `.to('cuda')`. Attention slicing
    and xformers memory-efficient attention are enabled for VRAM optimization.

    ! NOTE: `torch.compile()` and `enable_model_cpu_offload()` are disabled to prevent
    ! platform-specific conflicts on Windows (e.g., Triton dependency for torch.compile).

    This should be called once, then workers can clone from it.
    """
    global _compiled_master_pipeline

    if _compiled_master_pipeline is not None:
        console.print("[green]âœ“ Using cached compiled master pipeline[/green]")
        return _compiled_master_pipeline

    try:
        # * Suppress xformers/triton warnings during diffusers import
        with open(os.devnull, "w") as _f, contextlib.redirect_stderr(_f):
            from diffusers import AnimateDiffVideoToVideoPipeline
            from diffusers import ControlNetModel
            from diffusers import DDIMScheduler
            from diffusers import DPMSolverMultistepScheduler
            from diffusers import EulerAncestralDiscreteScheduler
            from diffusers import MotionAdapter
            from diffusers import StableDiffusionControlNetImg2ImgPipeline
            from diffusers import StableDiffusionImg2ImgPipeline
            from diffusers import UniPCMultistepScheduler

        console.print(
            f"[*] Setting up MASTER Stable Diffusion pipeline with model: {model_id}"
        )

        # * Validate model compatibility first
        if not validate_model_compatibility(model_id, controlnet_id):
            console.print(f"[red]! Model compatibility issue detected![/red]")
            console.print(f"[yellow]Base model: {model_id}[/yellow]")
            console.print(f"[yellow]ControlNet: {controlnet_id}[/yellow]")
            console.print(f"[cyan]Suggested compatible combinations:[/cyan]")
            console.print(f"[cyan]  â€¢ SD 1.5 + lllyasviel/sd-controlnet-depth[/cyan]")
            console.print(
                f"[cyan]  â€¢ SD 2.x + thibaud/controlnet-sd21-depth-diffusers[/cyan]"
            )
            raise typer.Exit(1)

        # * Ensure cache directories exist
        os.makedirs(os.environ["TORCH_COMPILE_CACHE_DIR"], exist_ok=True)
        os.makedirs(os.environ["TORCHINDUCTOR_CACHE_DIR"], exist_ok=True)

        # * Check if model_id is a local path or HF model ID
        if Path(model_id).exists():
            model_path = model_id
            console.print(f"[*] Using local model: {model_path}")
        else:
            # * Download model from Hugging Face Hub
            model_path = download_hf_model(model_id)
            console.print(f"[*] Downloaded model to: {model_path}")

        # * Initialize pipeline with memory optimizations
        device = "cuda" if torch.cuda.is_available() else "cpu"
        console.print(f"[*] Using device: {device}")

        if device == "cuda":
            # * Load pipeline on GPU while suppressing external warnings
            with (
                open(os.devnull, "w") as _devnull,
                contextlib.redirect_stderr(_devnull),
            ):
                if controlnet_id:
                    # * Load ControlNet model
                    console.print(f"[*] Loading ControlNet model: {controlnet_id}")
                    controlnet = ControlNetModel.from_pretrained(
                        controlnet_id,
                        torch_dtype=torch.float16,
                        use_safetensors=True,
                    )
                    # * Create ControlNet pipeline
                    pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
                        str(model_path),
                        controlnet=controlnet,
                        torch_dtype=torch.float16,
                        use_safetensors=True,
                        safety_checker=None,
                    ).to("cuda")
                    console.print("[green]âœ“ ControlNet pipeline loaded[/green]")
                else:
                    # * Create standard img2img pipeline
                    pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
                        str(model_path),
                        torch_dtype=torch.float16,
                        use_safetensors=True,
                        safety_checker=None,
                    ).to("cuda")
            # * Apply VRAM optimization modes
            if med_vram or low_vram:
                # * Enable attention slicing for VRAM optimization
                pipeline.enable_attention_slicing()
                console.print("[*] GPU attention slicing enabled for VRAM optimization")
            # * Low VRAM mode: enable CPU offload
            if low_vram:
                console.print("[*] Enabling model CPU offload for low VRAM usage")
                try:
                    pipeline.enable_model_cpu_offload()
                    console.print("[green]âœ“ Model CPU offload enabled[/green]")
                except AttributeError:
                    console.print(
                        "[yellow]! Model CPU offload not supported in this version of diffusers.[/yellow]"
                    )
                except Exception as e:
                    console.print(
                        f"[yellow]! Could not enable model CPU offload: {e}[/yellow]"
                    )
            pipeline.set_progress_bar_config(disable=True)
        else:
            # * Load CPU pipeline while suppressing external warnings
            with (
                open(os.devnull, "w") as _devnull,
                contextlib.redirect_stderr(_devnull),
            ):
                if controlnet_id:
                    # * Load ControlNet model for CPU
                    console.print(
                        f"[*] Loading ControlNet model for CPU: {controlnet_id}"
                    )
                    controlnet = ControlNetModel.from_pretrained(
                        controlnet_id,
                        torch_dtype=torch.float32,
                        use_safetensors=True,
                    )
                    # * Create ControlNet pipeline for CPU
                    pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
                        str(model_path),
                        controlnet=controlnet,
                        torch_dtype=torch.float32,
                        use_safetensors=True,
                        safety_checker=None,
                    )
                    console.print("[green]âœ“ ControlNet pipeline loaded on CPU[/green]")
                else:
                    # * Create standard img2img pipeline for CPU
                    pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
                        str(model_path),
                        torch_dtype=torch.float32,
                        use_safetensors=True,
                        safety_checker=None,
                    )
            console.print(
                "[yellow]! CUDA not available, using CPU (will be slow)[/yellow]"
            )
            pipeline.set_progress_bar_config(disable=True)

        # * Set the scheduler based on user choice
        console.print(f"[*] Setting scheduler to: {scheduler_name}")
        if scheduler_name == "UniPC":
            scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)
        elif scheduler_name == "DDIM":
            scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
        elif scheduler_name == "EulerA":
            scheduler = EulerAncestralDiscreteScheduler.from_config(
                pipeline.scheduler.config
            )
        elif scheduler_name == "DPM2M":
            scheduler = DPMSolverMultistepScheduler.from_config(
                pipeline.scheduler.config
            )
        else:  # Default to UniPC if invalid choice
            console.print(
                f"[yellow]! Invalid scheduler '{scheduler_name}', defaulting to UniPC.[/yellow]"
            )
            scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)

        pipeline.scheduler = scheduler
        # * The pipeline's device is managed by the offloader if enabled.
        # * Manually moving it again conflicts with the offloading mechanism.
        # pipeline.to(device)

        # * Cache the compiled pipeline
        _compiled_master_pipeline = pipeline
        console.print("[green]ðŸ’¾ MASTER pipeline cached for reuse[/green]")

        return pipeline

    except ImportError as e:
        console.print(f"[red]! Missing dependencies: {e}[/red]")
        console.print("[yellow]Run: pip install diffusers transformers torch[/yellow]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]! Error setting up master pipeline: {e}[/red]")
        raise typer.Exit(1)


def create_worker_pipeline() -> Any:
    """
    Create a worker pipeline by cloning the compiled master pipeline.

    Returns:
        A worker pipeline based on the compiled master
    """
    if _compiled_master_pipeline is None:
        raise ValueError("Master pipeline must be created first")

    try:
        import copy

        import torch

        # * Create a shallow copy of the master pipeline for this worker
        # ! Deep copy would duplicate the entire model in memory - we want to share weights
        console.print("[*] Creating worker pipeline from compiled master...")
        worker_pipeline = copy.copy(_compiled_master_pipeline)

        # * Clone only the critical components that need to be separate per worker
        worker_pipeline.unet = copy.copy(_compiled_master_pipeline.unet)
        worker_pipeline.scheduler = copy.deepcopy(_compiled_master_pipeline.scheduler)

        # * Ensure worker is on the correct device
        if torch.cuda.is_available():
            # ! Do not manually move to cuda. The worker pipeline inherits the
            # ! offloading mechanism from the master, which manages device placement.
            pass

        console.print("[green]âœ“ Worker pipeline created from compiled master[/green]")
        return worker_pipeline

    except Exception as e:
        console.print(f"[red]! Error creating worker pipeline: {e}[/red]")
        raise


async def _stylize_worker(
    frame_file: Path,
    stylized_dir: Path,
    pipeline: Any,
    prompt: str,
    strength: float,
    inference_steps: int,
    semaphore: asyncio.Semaphore,
    progress: Progress,
    task_id: TaskID,
    controlnet_id: Optional[str] = None,
    controlnet_strength: float = 1.0,
):
    """
    Worker to stylize a single frame, designed to be run concurrently.
    """
    import torch
    from PIL import Image

    try:
        async with semaphore:
            # Check for cancellation before starting work
            if progress.finished:
                return

            loop = asyncio.get_running_loop()

            # Load image (sync)
            input_image = Image.open(frame_file).convert("RGB")
            # * Resize to 512x512 for pipeline compatibility
            input_image = input_image.resize((512, 512), Image.Resampling.LANCZOS)

            # * Prepare ControlNet conditioning image if needed
            control_image = None
            if controlnet_id:
                # * Resize input image to 512x512 for ControlNet compatibility
                control_input = input_image.resize((512, 512), Image.Resampling.LANCZOS)

                if "depth" in controlnet_id.lower():
                    # * For depth ControlNet, use MiDaS depth detection
                    try:
                        midas = MidasDetector.from_pretrained("lllyasviel/Annotators")
                        depth_result = midas(control_input)
                        # * Ensure we get a PIL Image, not a tuple
                        control_image = (
                            depth_result
                            if isinstance(depth_result, Image.Image)
                            else (
                                depth_result[0]
                                if isinstance(depth_result, tuple)
                                else control_input.convert("L").convert("RGB")
                            )
                        )
                    except Exception:
                        # * Fallback to simple grayscale
                        control_image = control_input.convert("L").convert("RGB")
                elif "canny" in controlnet_id.lower():
                    # * For Canny ControlNet, use proper edge detection
                    try:
                        canny_detector = CannyDetector()
                        canny_result = canny_detector(control_input)
                        # * Ensure we get a PIL Image, not a tuple
                        control_image = (
                            canny_result
                            if isinstance(canny_result, Image.Image)
                            else (
                                canny_result[0]
                                if isinstance(canny_result, tuple)
                                else control_input
                            )
                        )
                    except Exception:
                        # * Fallback to OpenCV Canny
                        import cv2
                        import numpy as np

                        cv_image = cv2.cvtColor(
                            np.array(control_input), cv2.COLOR_RGB2BGR
                        )
                        canny = cv2.Canny(cv_image, 100, 200)
                        control_image = Image.fromarray(canny).convert("RGB")
                elif (
                    "openpose" in controlnet_id.lower()
                    or "pose" in controlnet_id.lower()
                ):
                    # * For OpenPose ControlNet, use pose detection
                    try:
                        openpose = OpenposeDetector.from_pretrained(
                            "lllyasviel/Annotators"
                        )
                        pose_result = openpose(control_input)
                        # * Ensure we get a PIL Image, not a tuple
                        control_image = (
                            pose_result
                            if isinstance(pose_result, Image.Image)
                            else (
                                pose_result[0]
                                if isinstance(pose_result, tuple)
                                else control_input
                            )
                        )
                    except Exception:
                        # * Fallback to original image
                        control_image = control_input
                else:
                    # * For other ControlNets, use the resized input image
                    control_image = control_input

            # * Prepare pipeline arguments
            pipe_kwargs = {
                "prompt": prompt,
                "image": input_image,
                "strength": strength,
                "guidance_scale": 7.5,
                "num_inference_steps": inference_steps,
            }

            if control_image is not None:
                pipe_kwargs["control_image"] = control_image
                pipe_kwargs["controlnet_conditioning_scale"] = controlnet_strength

            # Run blocking ML call in executor to not block the event loop
            func = functools.partial(pipeline, **pipe_kwargs)
            result = await loop.run_in_executor(None, func)

            output_file = stylized_dir / f"stylized_{frame_file.name}"
            result.images[0].save(output_file, "PNG")

            # Clear VRAM after each frame
            if hasattr(pipeline, "vae") and torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Update progress
            progress.update(task_id, advance=1)

    except Exception as e:
        console.print(f"[red]\n! Error processing frame {frame_file.name}: {e}[/red]")
        # Propagate the exception to be caught by asyncio.gather
        raise


async def stylize_frames(
    frames_dir: Path,
    stylized_dir: Path,
    pipelines: list[Any],
    prompt: str,
    strength: float = 0.6,
    inference_steps: int = 12,
    progress: Optional[Progress] = None,
    task_id: Optional[TaskID] = None,
    controlnet_id: Optional[str] = None,
    controlnet_strength: float = 1.0,
) -> bool:
    """
    Stylize extracted frames using Stable Diffusion concurrently.
    """
    try:
        ensure_dir_exists(stylized_dir)
        frame_files = sorted(list(frames_dir.glob("*.png")))
        if not frame_files:
            console.print("[red]! No frames found to stylize[/red]")
            return False

        if not progress or task_id is None:
            console.print("[red]! Progress bar not provided to stylize_frames[/red]")
            return False

        # * Run pipelines in parallel without semaphore: one async task per pipeline
        console.print(f"[*] Running {len(pipelines)} parallel pipelines...")
        loop = asyncio.get_running_loop()

        async def pipeline_loop(pipeline: Any, files: list[Path]):
            for f in files:
                # Load and preprocess image
                img = Image.open(f).convert("RGB")
                img = img.resize((512, 512), Image.Resampling.LANCZOS)
                # Build pipeline kwargs
                pipe_kwargs = {
                    "prompt": prompt,
                    "image": img,
                    "strength": strength,
                    "guidance_scale": 7.5,
                    "num_inference_steps": inference_steps,
                }
                if controlnet_id:
                    pipe_kwargs["control_image"] = img
                    pipe_kwargs["controlnet_conditioning_scale"] = controlnet_strength
                # Run inference
                func = functools.partial(pipeline, **pipe_kwargs)
                result = await loop.run_in_executor(None, func)
                # Save output frame
                out_path = stylized_dir / f"stylized_{f.name}"
                result.images[0].save(out_path, "PNG")  # type: ignore
                if hasattr(pipeline, "vae") and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                progress.update(task_id, advance=1)

        # Assign frames round-robin and start tasks
        num = len(pipelines)
        tasks = []
        for idx, pipeline in enumerate(pipelines):
            subset = [f for i, f in enumerate(frame_files) if i % num == idx]
            tasks.append(asyncio.create_task(pipeline_loop(pipeline, subset)))
        try:
            await asyncio.gather(*tasks)
            return True
        except Exception as e:
            console.print(f"[red]! Parallel stylization error: {e}[/red]")
            return False

    except ImportError as e:
        console.print(f"[red]! Missing dependencies: {e}[/red]")
        console.print("[yellow]Run: pip install Pillow torch[/yellow]")
        return False
    except Exception:
        # The specific error is already printed in the worker
        console.print(f"\n[red]! A critical error occurred during stylization.[/red]")
        return False


async def get_video_framerate(input_video: Path) -> float:
    """
    Get video framerate using ffprobe.

    Args:
        input_video: Path to the input video

    Returns:
        Video framerate as float, defaults to 25.0 if detection fails
    """
    try:
        ffprobe_cmd = [
            "ffprobe",
            "-v",
            "error",
            "-select_streams",
            "v:0",
            "-show_entries",
            "stream=r_frame_rate",
            "-of",
            "csv=p=0",
            str(input_video),
        ]

        proc = await asyncio.create_subprocess_exec(
            *ffprobe_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode == 0 and stdout:
            framerate_str = stdout.decode().strip()
            # Handle fractional framerates like "30000/1001"
            if "/" in framerate_str:
                num, den = framerate_str.split("/")
                framerate = float(num) / float(den)
            else:
                framerate = float(framerate_str)

            console.print(f"[*] Detected framerate: {framerate:.2f} fps")
            return framerate
        else:
            console.print(
                "[yellow]! Could not detect framerate, using 25.0 fps[/yellow]"
            )
            return 25.0

    except Exception as e:
        console.print(
            f"[yellow]! Error detecting framerate: {e}, using 25.0 fps[/yellow]"
        )
        return 25.0


async def reassemble_video(
    frames_dir: Path,
    output_path: Path,
    source_video_path: Path,
    frame_rate: float,
    progress: Optional[Progress] = None,
    task_id: Optional[TaskID] = None,
) -> bool:
    """
    Reassemble frames into video using ffmpeg with HEVC CQ=26 and audio copy.

    Args:
        frames_dir: Directory containing frames to reassemble
        output_path: Output video file path
        source_video_path: Original input video for framerate detection
        frame_rate: Detected video framerate
        progress: Rich progress bar instance
        task_id: Rich progress bar task ID

    Returns:
        True if successful, False otherwise
    """
    try:
        # * Get list of frame files and ensure they exist
        frame_files = sorted(list(frames_dir.glob("*.png")))
        if not frame_files:
            console.print("[red]! No frames found for video reassembly[/red]")
            return False

        console.print(f"[*] Reassembling {len(frame_files)} frames into video...")

        # * Build ffmpeg command for HEVC encoding with CQ=26 and audio copy
        ffmpeg_cmd = [
            "ffmpeg",
            "-y",  # Overwrite output
            "-framerate",
            str(frame_rate),
            "-i",
            str(frames_dir / "stylized_%06d.png"),  # Input pattern (video)
            "-i",
            str(source_video_path),  # Original video source for audio
            "-map",
            "0:v:0",  # Use newly encoded video stream
            "-map",
            "1:a?",  # Copy all audio streams if present
            "-c:v",
            "libx265",
            "-crf",
            "26",
            "-preset",
            "medium",
            "-pix_fmt",
            "yuv420p",
            "-c:a",
            "copy",
            "-movflags",
            "+faststart",
            "-shortest",  # Ensure output length matches shortest stream
            str(output_path),
        ]

        console.print("[*] Encoding with HEVC CQ=26...")

        # * Run ffmpeg command
        proc = await asyncio.create_subprocess_exec(
            *ffmpeg_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode != 0:
            console.print(
                f"[red]! FFmpeg error during reassembly: {stderr.decode()}[/red]"
            )
            return False

        console.print("[green]âœ“ Video reassembled successfully[/green]")
        return True

    except Exception as e:
        console.print(f"[red]! Error during video reassembly: {e}[/red]")
        return False


async def upscale_frames_realesrgan(stylized_dir: Path, upscaled_dir: Path) -> bool:
    """
    Upscale stylized frames using Real-ESRGAN.

    Args:
        stylized_dir: Directory containing stylized frames
        upscaled_dir: Directory to save upscaled frames

    Returns:
        True if successful, False otherwise
    """
    try:
        # * Ensure output directory exists
        ensure_dir_exists(upscaled_dir)

        # * Get list of stylized frame files
        frame_files = sorted(list(stylized_dir.glob("*.png")))
        if not frame_files:
            console.print("[red]! No stylized frames found for upscaling[/red]")
            return False

        console.print(f"[*] Upscaling {len(frame_files)} frames with Real-ESRGAN...")

        # * Try to import Real-ESRGAN
        try:
            import cv2
            from realesrgan import RealESRGANer
            from realesrgan.archs.rrdbnet_arch import RRDBNet  # type: ignore
        except ImportError as e:
            console.print(f"[red]! Missing Real-ESRGAN dependencies: {e}[/red]")
            console.print("[yellow]Run: pip install realesrgan opencv-python[/yellow]")
            return False

        # * Initialize Real-ESRGAN upscaler using RRDBNet arch for x4plus weights
        model = RRDBNet(num_in_ch=3, num_out_ch=3, nf=64, nb=23, upscale=4)

        upsampler = RealESRGANer(
            scale=4,
            model_path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
            model=model,
            tile=0,
            tile_pad=10,
            pre_pad=0,
            half=True,  # * Use FP16 for faster processing and lower VRAM usage
        )

        # * Process each frame
        for i, frame_file in enumerate(frame_files):
            console.print(
                f"[*] Upscaling frame {i+1}/{len(frame_files)}: {frame_file.name}"
            )

            # * Load image
            img = cv2.imread(str(frame_file), cv2.IMREAD_COLOR)

            # * Upscale with Real-ESRGAN
            output, _ = upsampler.enhance(img, outscale=4)

            # * Save upscaled frame
            output_file = upscaled_dir / f"upscaled_{frame_file.name}"
            cv2.imwrite(str(output_file), output)

        console.print(
            f"[green]âœ“ Successfully upscaled {len(frame_files)} frames[/green]"
        )
        return True

    except Exception as e:
        console.print(f"[red]! Error during Real-ESRGAN upscaling: {e}[/red]")
        return False


async def stylize_video_animatediff(
    frames_dir: Path,
    stylized_dir: Path,
    prompt: str,
    model_id: str,
    controlnet_id: Optional[str] = None,
    inference_steps: int = 25,
    strength: float = 0.6,
    progress: Optional[Progress] = None,
    task_id: Optional[TaskID] = None,
    med_vram: bool = False,
    low_vram: bool = False,
    controlnet_strength: float = 1.0,
):
    """
    Stylizes video frames using AnimateDiff for temporal consistency, with optional ControlNet.

    This function processes the video in overlapping chunks to handle videos longer
    than the model's context window.

    Args:
        frames_dir: Directory with input frames.
        stylized_dir: Directory to save stylized frames.
        prompt: Text prompt for stylization.
        model_id: HuggingFace model ID for the base diffusion model.
        controlnet_id: Optional HuggingFace model ID for ControlNet.
        inference_steps: Number of diffusion steps.
        strength: Stylization strength.
        progress: Rich progress bar instance.
        task_id: Rich progress bar task ID.
        med_vram: Enable medium VRAM optimizations (attention slicing & xformers).
        low_vram: Enable low VRAM optimizations (model CPU offload).
        controlnet_strength: Strength of the ControlNet applied to the video.

    Returns:
        True if successful, False otherwise.
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        console.print(
            "[yellow]! Warning: Running on CPU, this will be very slow.[/yellow]"
        )

    # * Local import to satisfy static analyzers (avoids false positives)
    try:
        from diffusers import (  # type: ignore # noqa
            AnimateDiffVideoToVideoControlNetPipeline,
        )
        from diffusers import AnimateDiffVideoToVideoPipeline
        from diffusers import ControlNetModel
        from diffusers import DDIMScheduler
        from diffusers import MotionAdapter

        console.print(
            "[green]âœ“ Using AnimateDiffVideoToVideoPipeline for video-to-video stylization[/green]"
        )
        is_video_to_video = True
    except ImportError:
        # * Fallback: use regular img2img with motion adapter
        console.print(
            "[yellow]! AnimateDiffVideoToVideoPipeline not available, using img2img approach[/yellow]"
        )
        from diffusers import ControlNetModel  # type: ignore
        from diffusers import DDIMScheduler
        from diffusers import MotionAdapter
        from diffusers import StableDiffusionImg2ImgPipeline

        AnimateDiffVideoToVideoPipeline = StableDiffusionImg2ImgPipeline  # type: ignore
        AnimateDiffVideoToVideoControlNetPipeline = None  # type: ignore
        is_video_to_video = False
    from PIL import Image

    try:
        # * Stylization of video frames with AnimateDiff
        console.print("[bold cyan]Starting video stylization...[/bold cyan]")

        # --- 1. Load models and pipeline ---
        if progress and task_id:
            progress.update(task_id, description="Loading models...")

        adapter = MotionAdapter.from_pretrained(
            "guoyww/animatediff-motion-adapter-v1-5-2",
            torch_dtype=torch.float16,
        )

        controlnet = None
        if controlnet_id:
            console.print(f"[*] Loading ControlNet: {controlnet_id}")
            controlnet = ControlNetModel.from_pretrained(
                controlnet_id, torch_dtype=torch.float16
            )

        # * Build the pipeline. Some versions of diffusers do not accept the `controlnet` keyword for
        # * `AnimateDiffPipeline.from_pretrained`. To avoid the "Keyword arguments { ... } are not expected"
        # * console spam, we conditionally attach ControlNet _after_ the pipeline is instantiated when
        # * the constructor does not advertise explicit support.

        if is_video_to_video:
            # Prefer ControlNet pipeline if available
            if (
                controlnet is not None
                and AnimateDiffVideoToVideoControlNetPipeline is not None
            ):
                console.print(
                    "[green]âœ“ Using AnimateDiffVideoToVideoControlNetPipeline with ControlNet[/green]"
                )
                pipe = AnimateDiffVideoToVideoControlNetPipeline.from_pretrained(
                    model_id,
                    motion_adapter=adapter,
                    controlnet=controlnet,
                    torch_dtype=torch.float16,
                ).to(device)
            elif controlnet is not None:
                console.print(
                    "[yellow]! ControlNet video-to-video pipeline not available, using base AnimateDiffVideoToVideoPipeline[/yellow]"
                )
                pipe = AnimateDiffVideoToVideoPipeline.from_pretrained(
                    model_id,
                    motion_adapter=adapter,
                    torch_dtype=torch.float16,
                ).to(device)
            else:
                console.print(
                    "[green]âœ“ Using AnimateDiffVideoToVideoPipeline for video-to-video stylization[/green]"
                )
                pipe = AnimateDiffVideoToVideoPipeline.from_pretrained(
                    model_id,
                    motion_adapter=adapter,
                    torch_dtype=torch.float16,
                ).to(device)
        else:
            # * Fallback: use img2img pipeline without temporal consistency
            console.print("[yellow]âœ“ Using img2img fallback for AnimateDiff[/yellow]")
            pipe = AnimateDiffVideoToVideoPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
            ).to(device)

        # * Apply VRAM optimization modes for AnimateDiff
        if device == "cuda":
            if med_vram or low_vram:
                pipe.enable_attention_slicing()
                console.print(
                    "[*] AnimateDiff attention slicing enabled for VRAM optimization"
                )
            if low_vram:
                console.print(
                    "[*] Enabling AnimateDiff model CPU offload for low VRAM usage"
                )
                try:
                    pipe.enable_model_cpu_offload()
                    console.print(
                        "[green]âœ“ AnimateDiff model CPU offload enabled[/green]"
                    )
                except Exception:
                    console.print(
                        "[yellow]! AnimateDiff model CPU offload not supported by this pipeline.[/yellow]"
                    )

        pipe.scheduler = DDIMScheduler.from_config(
            pipe.scheduler.config,
            beta_schedule="linear",
            steps_offset=1,
            clip_sample=False,
        )
        pipe.set_progress_bar_config(disable=True)
        if is_video_to_video:
            console.print(
                "[green]âœ” AnimateDiff video-to-video pipeline loaded - input frames will be used as reference.[/green]"
            )
        else:
            console.print(
                "[yellow]âœ” Using img2img fallback - processing frames individually (no temporal consistency).[/yellow]"
            )
            console.print(
                "[yellow]Consider updating diffusers for proper AnimateDiff video-to-video support.[/yellow]"
            )

        # --- Negative prompt setup ---
        neg_prompt_string = "(((bad_quality:0.75), (bad_prompt_version2:0.75), negprompt5, verybadimagenegative_v1.3):0.9)"
        neg_embeds = None
        try:
            emb_dir = Path(__file__).parent / "Embeddings"
            emb1 = torch.load(emb_dir / "bad_quality.pt", map_location=device)
            emb2 = torch.load(emb_dir / "bad_prompt_version2.pt", map_location=device)
            emb3 = torch.load(emb_dir / "negprompt5.pt", map_location=device)
            emb4 = torch.load(
                emb_dir / "verybadimagenegative_v1.3.pt", map_location=device
            )
            neg_embeds = emb1 * 0.75 + emb2 * 0.75 + emb3 + emb4
            neg_embeds = neg_embeds * 0.9
            console.print("[*] Loaded negative embeddings from Embeddings folder")
        except Exception as e:
            console.print(
                f"[yellow]! Could not load negative embeddings ({e}); using text negative prompt[/yellow]"
            )

        # --- 2. Prepare frames and ControlNet conditions ---
        if progress and task_id:
            progress.update(task_id, description="Preparing frames...")

        ensure_dir_exists(stylized_dir)
        frame_files = sorted(frames_dir.glob("*.png"))
        video_frames = [Image.open(f).convert("RGB") for f in frame_files]
        # * Preserve original frame resolution for output
        orig_size = video_frames[0].size if video_frames else None

        controlnet_frames = None
        if controlnet_id and controlnet:
            console.print("[*] Pre-processing frames for ControlNet...")
            # Select the appropriate preprocessor based on the model ID
            if "canny" in controlnet_id:
                detector = CannyDetector()
            elif "openpose" in controlnet_id:
                detector = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
            elif "depth" in controlnet_id:
                detector = MidasDetector.from_pretrained("lllyasviel/Annotators")
                # * Move detector to device after initialization
                if hasattr(detector, "to"):
                    detector = detector.to(device)
            else:
                console.print(
                    f"[yellow]! Warning: No specific preprocessor found for {controlnet_id}. Using Canny as default.[/yellow]"
                )
                detector = CannyDetector()

            controlnet_frames = [
                detector(img, detect_resolution=384, image_resolution=512)
                for img in video_frames
            ]

        # --- 3. Process video in chunks ---
        context_size = 16  # AnimateDiff standard context window
        overlap = 4  # Number of frames to overlap between chunks
        total_frames = len(video_frames)
        stylized_frames: list[Image.Image | None] = [None] * total_frames

        if progress and task_id:
            progress.update(
                task_id,
                total=total_frames,
                completed=0,
                description="Stylizing video chunks...",
            )

        # * Define a callback to update progress bar with inner step progress
        def progress_bar_callback(
            _pipeline, step: int, timestep: float, latents: torch.Tensor
        ):
            # * Inner-step progress callback: pipeline, step index, timestep, latents
            if progress and task_id:
                progress.update(
                    task_id,
                    description=f"Chunk {start_idx + 1}-{end_idx}: Denoising step {step + 1}/{inference_steps}",
                )
            # Return a dict so pipeline can pop safely
            return {}

        start_idx = 0
        while start_idx < total_frames:
            end_idx = min(start_idx + context_size, total_frames)
            chunk_frames = video_frames[start_idx:end_idx]

            if progress and task_id:
                progress.update(
                    task_id,
                    description=f"Processing chunk: frames {start_idx + 1}-{end_idx}...",
                )
            else:
                console.print(
                    f"[*] Processing chunk: frames {start_idx + 1}-{end_idx}..."
                )

            if is_video_to_video:
                # * Use video-to-video pipeline with input frames
                pipe_kwargs = {
                    "prompt": prompt,
                    "video": chunk_frames,
                    "num_inference_steps": inference_steps,
                    "strength": strength,
                    "guidance_scale": 7.5,
                    "callback_on_step_end": progress_bar_callback,
                }

                if controlnet_frames:
                    pipe_kwargs["conditioning_frames"] = controlnet_frames[start_idx:end_idx]  # type: ignore
                    pipe_kwargs["controlnet_conditioning_scale"] = controlnet_strength

                # * Inject negative prompt or embeddings
                if neg_embeds is not None:
                    pipe_kwargs["negative_prompt_embeds"] = neg_embeds
                else:
                    pipe_kwargs["negative_prompt"] = neg_prompt_string

                # * Run pipeline and extract raw output frames
                raw_output = pipe(**pipe_kwargs).frames[0]  # type: ignore
            else:
                # * Fallback: process frames individually with img2img
                raw_output = []
                for i, frame in enumerate(chunk_frames):
                    if controlnet_frames:
                        control_img = controlnet_frames[start_idx + i]
                        pipe_kwargs = {
                            "prompt": prompt,
                            "image": frame,
                            "control_image": control_img,
                            "num_inference_steps": inference_steps,
                            "strength": strength,
                            "guidance_scale": 7.5,
                            "controlnet_conditioning_scale": controlnet_strength,
                        }
                    else:
                        pipe_kwargs = {
                            "prompt": prompt,
                            "image": frame,
                            "num_inference_steps": inference_steps,
                            "strength": strength,
                            "guidance_scale": 7.5,
                        }

                    # * Inject negative prompt or embeddings
                    if neg_embeds is not None:
                        pipe_kwargs["negative_prompt_embeds"] = neg_embeds
                    else:
                        pipe_kwargs["negative_prompt"] = neg_prompt_string

                    result = pipe(**pipe_kwargs)  # type: ignore
                    raw_output.append(result.images[0])
            # * Ensure output length matches input chunk length to avoid index errors
            raw_len = len(raw_output)
            chunk_len = len(chunk_frames)
            if raw_len != chunk_len:
                console.print(
                    f"[yellow]! Warning: expected {chunk_len} frames, got {raw_len}. Truncating to match input length.[/yellow]"
                )
            output = raw_output[:chunk_len]

            # Blend overlapping frames to smooth transitions
            for i in range(len(output)):  # type: ignore
                frame_idx = start_idx + i
                current_stylized_frame = stylized_frames[frame_idx]
                if current_stylized_frame is not None:
                    # Blend with the previous frame's stylized version
                    alpha = (i - (context_size - overlap)) / overlap
                    blended_frame = Image.blend(current_stylized_frame, output[i], alpha)  # type: ignore
                    stylized_frames[frame_idx] = blended_frame
                else:
                    stylized_frames[frame_idx] = output[i]  # type: ignore

            start_idx += context_size - overlap
            if start_idx >= total_frames - overlap:  # Ensure last frames are processed
                start_idx = max(0, total_frames - context_size)
                if end_idx == total_frames:
                    break

        # --- 4. Save frames with resolution restore and per-frame progress ---
        if progress and task_id:
            progress.update(task_id, description="Saving frames...", completed=0)
        for idx, frame in enumerate(stylized_frames):
            if frame and orig_size:
                out_path = stylized_dir / f"stylized_{idx:06d}.png"
                # * Restore original resolution
                frame = frame.resize(orig_size, Image.Resampling.LANCZOS)
                frame.save(out_path, "PNG")
                # * Update progress per frame
                if progress and task_id:
                    progress.update(task_id, advance=1)

    except Exception as e:
        console.print(f"[bold red]! AnimateDiff stylization failed: {e}[/bold red]")
        import traceback

        traceback.print_exc()
        return False

    return True


async def run_stylization(
    video_path: Path,
    prompt: str,
    model_id: str,
    controlnet_id: Optional[str],
    use_animatediff: bool,
    strength: float,
    inference_steps: int,
    scheduler: str,
    upscale: bool,
    med_vram: bool,
    low_vram: bool,
    output: Path,
    max_resolution: str,
    fps: int,
    test_mode: bool,
    work_dir: Optional[Path] = None,
    controlnet_strength: float = 1.0,
    concurrency_limit: int = 2,
):
    """
    Run the asynchronous stylization pipeline.
    """
    # * Stylization of video frames with AnimateDiff
    console.print("[bold cyan]Starting video stylization...[/bold cyan]")

    console.print(f"Input: [yellow]{video_path}[/yellow]")
    console.print(f"Prompt: [green]'{prompt}'[/green]")
    console.print(f"Model: [blue]{model_id}[/blue]")

    if use_animatediff:
        console.print("[*] AnimateDiff mode: [green]Enabled[/green]")
    else:
        console.print("[*] AnimateDiff mode: [yellow]Disabled[/yellow]")

    if controlnet_id:
        console.print(f"[*] ControlNet model: [green]{controlnet_id}[/green]")
    else:
        console.print("[*] ControlNet model: [yellow]Disabled[/yellow]")

    # * Create temporary directories; allow custom work_dir to host temp files
    temp_dir_kwargs: dict[str, Any] = {"prefix": "stylizer_"}
    if work_dir:
        temp_dir_kwargs["dir"] = work_dir
    with tempfile.TemporaryDirectory(**temp_dir_kwargs) as temp_dir:
        temp_path = Path(temp_dir)
        frames_dir = temp_path / "frames"
        stylized_dir = temp_path / "stylized_frames"
        upscaled_dir = temp_path / "upscaled_frames"
        final_frames_dir = stylized_dir  # Will be upscaled_dir if upscaling enabled

        console.print(f"[dim]Working directory: {temp_path}[/dim]")

        # * Step 1: Extract frames
        console.print("\n[bold]Step 1: Extracting frames[/bold]")
        success = await extract_frames(
            video_path, frames_dir, max_resolution, test_mode, fps
        )
        if not success:
            console.print("[red]! Failed to extract frames[/red]")
            raise typer.Exit(1)

        # * Step 2: Stylizing video with AnimateDiff
        if use_animatediff:
            console.print("\n[bold]Step 2: Stylizing video with AnimateDiff[/bold]")
            # * AnimateDiff with progress bar integration
            total_frames = len(list(frames_dir.glob("*.png")))
            with Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                console=console,
                transient=True,
                refresh_per_second=2,
            ) as progress_bar:
                task = progress_bar.add_task(
                    "Stylizing video chunks...", total=total_frames
                )
                success = await stylize_video_animatediff(
                    frames_dir,
                    stylized_dir,
                    prompt,
                    model_id,
                    controlnet_id,
                    inference_steps,
                    strength,
                    progress_bar,
                    task,
                    med_vram,
                    low_vram,
                    controlnet_strength,
                )
            if not success:
                console.print("[red]! AnimateDiff stylization failed[/red]")
                raise typer.Exit(1)
        else:
            # * Original per-frame stylization path
            console.print(
                f"\n[bold]Step 2: Setting up master pipeline and {concurrency_limit} worker pipelines[/bold]"
            )

            # * If parallel pipelines and CPU offload are both enabled, disable CPU offload to keep modules on the same device
            if low_vram and concurrency_limit > 1:
                console.print(
                    "[yellow]! Parallel pipelines incompatible with low_vram CPU offload. Disabling model CPU offload.[/yellow]"
                )
                low_vram = False

            master_pipeline = setup_master_compiled_pipeline(
                model_id, scheduler, med_vram, low_vram, controlnet_id
            )
            pipelines = [create_worker_pipeline() for _ in range(concurrency_limit)]

            if controlnet_id:
                console.print(f"[*] Integrating ControlNet: {controlnet_id}")
                # * ControlNet integration is handled in the pipeline setup
                # * The master pipeline needs to be recreated with ControlNet support

            console.print("\n[bold]Step 3: Stylizing frames[/bold]")
            console.print(f"[*] Prompt: '{prompt}'")
            console.print(f"[*] Strength: {strength}")

            with Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                console=console,
                transient=True,
                refresh_per_second=2,
            ) as progress_bar:
                task = progress_bar.add_task(
                    "Stylizing frames...", total=len(list(frames_dir.glob("*.png")))
                )
                success = await stylize_frames(
                    frames_dir,
                    stylized_dir,
                    pipelines,
                    prompt,
                    strength,
                    inference_steps,
                    progress_bar,
                    task,
                    controlnet_id,
                    controlnet_strength,
                )
                if not success:
                    console.print("[red]! Failed to stylize frames[/red]")
                    raise typer.Exit(1)

        # * Step 4: Upscaling (optional)
        if upscale:
            console.print("\n[bold]Step 4: Upscaling frames (Real-ESRGAN)[/bold]")
            success = await upscale_frames_realesrgan(stylized_dir, upscaled_dir)
            if success:
                final_frames_dir = upscaled_dir
            else:
                console.print(
                    "[yellow]! Real-ESRGAN upscaling failed, using stylized frames without upscaling[/yellow]"
                )
        else:
            console.print("\n[dim]Skipping upscaling (--upscale not enabled)[/dim]")

        # * Step 5: Reassemble video
        console.print("\n[bold]Step 5: Reassembling video[/bold]")
        success = await reassemble_video(
            final_frames_dir,
            output,
            video_path,
            await get_video_framerate(video_path),
            None,
            None,
        )
        if not success:
            console.print("[red]! Failed to reassemble video[/red]")
            raise typer.Exit(1)

        console.print("\n[bold green]âœ“ Video stylization completed![/bold green]")
        console.print(f"[green]Output saved to: {output}[/green]")

        # * Print summary statistics
        console.print("\n[bold underline]Summary[/bold underline]")
        console.print(f"Prompt: [italic]{prompt}[/italic]")
        console.print(f"Model: {model_id}")
        if controlnet_id:
            console.print(f"ControlNet: {controlnet_id}")
        console.print(f"AnimateDiff: {use_animatediff}")
        console.print(f"Strength: {strength}")
        console.print(f"Inference steps: {inference_steps}")
        console.print(f"Scheduler: {scheduler}")
        console.print(f"Upscale: {upscale}")
        console.print(f"Resolution: {max_resolution}")


@app.command()
def stylize(
    input_video: Annotated[
        Optional[Path], typer.Argument(help="Input video file.")
    ] = None,
    prompt: Annotated[
        Optional[str],
        typer.Option("--prompt", "-p", help="Text prompt for stylization."),
    ] = None,
    model_id: Annotated[
        str,
        typer.Option(
            "--model-id",
            "-m",
            help="Hugging Face model ID for Stable Diffusion (e.g., runwayml/stable-diffusion-v1-5).",
        ),
    ] = "runwayml/stable-diffusion-v1-5",
    controlnet_id: Annotated[
        Optional[str],
        typer.Option(
            "--controlnet-id",
            "-c",
            help="Hugging Face model ID for ControlNet (optional).",
        ),
    ] = "lllyasviel/sd-controlnet-depth",
    use_animatediff: Annotated[
        bool,
        typer.Option(
            "--animatediff", help="Enable AnimateDiff for temporal consistency."
        ),
    ] = False,
    strength: Annotated[
        Optional[float],
        typer.Option(
            "--strength", "-s", min=0.0, max=1.0, help="Stylization strength (0.0-1.0)."
        ),
    ] = None,
    inference_steps: Annotated[
        Optional[int],
        typer.Option("--steps", help="Number of inference steps for Stable Diffusion."),
    ] = None,
    scheduler: Annotated[
        str,
        typer.Option(
            "--scheduler",
            help="Scheduler to use: UniPC, DDIM, EulerA, DPM2M.",
        ),
    ] = "UniPC",
    upscale: Annotated[
        bool,
        typer.Option(
            "--upscale",
            help="Enable Real-ESRGAN 4x upscaling to FHD (currently broken, use Topaz).",
        ),
    ] = False,
    med_vram: Annotated[
        bool,
        typer.Option(
            "--med-vram",
            help="Enable medium VRAM optimizations (attention slicing & xformers).",
        ),
    ] = False,
    low_vram: Annotated[
        bool,
        typer.Option(
            "--low-vram",
            help="Enable low VRAM optimizations (model CPU offload). Overrides med-vram.",
        ),
    ] = True,
    fps: Annotated[
        int,
        typer.Option("--fps", "-f", help="Frames per second to process (15 or 30)."),
    ] = 15,
    output: Annotated[
        Optional[Path], typer.Option("--output", "-o", help="Output video file path.")
    ] = None,
    max_resolution: Annotated[
        str,
        typer.Option(
            "--max-res",
            help="Maximum resolution for processing (e.g., '480p', '720p').",
        ),
    ] = "480p",
    work_dir: Annotated[
        Optional[Path],
        typer.Option(
            "--work-dir",
            "-w",
            help="Directory for temporary files (defaults to system temp).",
        ),
    ] = None,
    test_mode: Annotated[
        bool,
        typer.Option(
            "--test-mode",
            help="Enable test mode (process first 2 seconds only).",
        ),
    ] = False,
):
    """
    Stylize a video using Stable Diffusion and optionally upscale with Real-ESRGAN.

    The process involves:
    1. Extract frames from video at specified resolution
    2. Stylize frames using Stable Diffusion
    3. Optionally upscale frames using Real-ESRGAN
    4. Reassemble video with HEVC encoding
    """
    # * Interactive prompt for input video
    if input_video is None:
        input_video = prompt_for_path(
            "Enter input video file:",
            default=None,
            must_exist=True,
            file_okay=True,
            dir_okay=False,
        )
    # * Validate input file exists
    if not input_video.exists():
        console.print(f"[red]! Error: Input video not found: {input_video}[/red]")
        raise typer.Exit(1)

    # * Interactive prompt for stylization prompt
    if prompt is None:
        prompt = typer.prompt("Enter text prompt for stylization:")
    # * Ensure prompt is set
    assert prompt is not None

    # * --- Interactive Settings Menu ---
    # * Determine compatible ControlNets based on base model
    if "stable-diffusion-2" in model_id:
        # SD 2.x compatible ControlNets
        controlnet_choices = {
            "Disabled": "",
            "Canny (SD2)": "thibaud/controlnet-sd21-canny-diffusers",
            "Depth (SD2)": "thibaud/controlnet-sd21-depth-diffusers",
            "OpenPose (SD2)": "thibaud/controlnet-sd21-openpose-diffusers",
        }
    else:
        # SD 1.5 compatible ControlNets (default)
        controlnet_choices = {
            "Disabled": "",
            "Canny": "lllyasviel/sd-controlnet-canny",
            "Depth": "lllyasviel/sd-controlnet-depth",
            "OpenPose": "lllyasviel/sd-controlnet-openpose",
        }

    settings_definitions = [
        {
            "key": "scheduler",
            "label": "Scheduler",
            "type": "choice",
            "choices": {
                "UniPC": "UniPC",
                "DDIM": "DDIM",
                "EulerA": "EulerA",
                "DPM2M": "DPM2M",
            },
        },
        {"key": "use_animatediff", "label": "AnimateDiff (temporal)", "type": "toggle"},
        {
            "key": "controlnet_id",
            "label": "ControlNet (structural)",
            "type": "choice",
            "choices": controlnet_choices,
        },
        {
            "key": "controlnet_strength",
            "label": "ControlNet Strength",
            "type": "choice",
            "choices": {
                "0.4": "0.4",
                "0.6": "0.6",
                "0.8": "0.8",
                "1.0": "1.0",
                "1.2": "1.2",
            },
        },
        {
            "key": "strength",
            "label": "Style Strength",
            "type": "choice",
            "choices": {
                "0.2": "0.2",
                "0.4": "0.4",
                "0.6": "0.6",
                "0.8": "0.8",
                "1.0": "1.0",
            },
        },
        {
            "key": "inference_steps",
            "label": "Inference Steps",
            "type": "choice",
            "choices": {"8": "8", "12": "12", "16": "16", "25": "25", "30": "30"},
        },
        {
            "key": "fps",
            "label": "FPS",
            "type": "choice",
            "choices": {"15": "15", "30": "30"},
        },
        {"key": "upscale", "label": "4x Upscale (Real-ESRGAN)", "type": "toggle"},
        {"key": "test_mode", "label": "Test Mode (First 2s)", "type": "toggle"},
        {
            "key": "concurrency_limit",
            "label": "Parallel Pipelines (CONCURRENCY_LIMIT)",
            "type": "choice",
            "choices": {"1": "1", "2": "2", "3": "3", "4": "4"},
        },
    ]

    # * Get initial values from CLI for interactive settings
    current_settings = {
        "scheduler": scheduler,
        "use_animatediff": use_animatediff,
        "controlnet_id": controlnet_id or "",
        "controlnet_strength": "1.0",  # Default ControlNet strength
        "strength": str(strength if strength is not None else 0.6),
        "inference_steps": str(inference_steps if inference_steps is not None else 12),
        "fps": str(fps),
        "upscale": upscale,
        "test_mode": test_mode,
        "concurrency_limit": "2",
    }

    # * Always show interactive settings so user can tweak parameters quickly
    is_interactive = True

    final_settings = None
    if is_interactive:
        console.print("\n[bold cyan]ðŸŽ¨ Stylizer Settings[/bold cyan]")
        final_settings = prompt_for_interactive_settings(
            settings_definitions, current_settings, title="Stylizer Settings"
        )
        if final_settings is None:
            console.print("[yellow]! Operation cancelled by user.[/yellow]")
            raise typer.Exit()
    else:
        final_settings = current_settings

    # * Extract final values from interactive settings
    final_scheduler = str(final_settings["scheduler"])
    final_animatediff = bool(final_settings["use_animatediff"])
    final_controlnet_id = (
        str(final_settings["controlnet_id"])
        if final_settings["controlnet_id"]
        else None
    )
    final_controlnet_strength = float(final_settings["controlnet_strength"])
    final_strength = float(final_settings["strength"])
    final_steps = int(final_settings["inference_steps"])
    final_fps = int(final_settings["fps"])
    final_upscale = bool(final_settings["upscale"])
    final_test_mode = bool(final_settings["test_mode"])
    final_med_vram = med_vram
    final_low_vram = low_vram
    final_concurrency_limit = int(final_settings["concurrency_limit"])

    # * Set up output path
    if output is None:
        output = input_video.parent / f"{input_video.stem}_stylized.mp4"

    console.print(f"Output: [yellow]{output}[/yellow]")

    # * Run the async pipeline
    asyncio.run(
        run_stylization(
            input_video,
            prompt,
            model_id,
            final_controlnet_id,
            final_animatediff,
            final_strength,
            final_steps,
            final_scheduler,
            final_upscale,
            final_med_vram,
            final_low_vram,
            output,
            max_resolution,
            final_fps,
            final_test_mode,
            work_dir,
            final_controlnet_strength,
            final_concurrency_limit,
        )
    )


if __name__ == "__main__":
    app()
